\section{Novelty Search Compiler Optimizations Exploration}
 
The goal of the novelty search approach for compiler optimization, introduced by Lehman and Stanley in 2008
[22], is to identify a set of compiler optimization levels that provide a trade-off with respect to resource utilization. 

\subsection{Novelty Search Adaptation}
Optimization options are difficult and even impossible to be chosen by programmers or compiler users. So a tool to help users to choose the best set of options becomes necessary to achieve a compiler optimization with effectiveness and efficiently. 

There have been many previous works that have investigated this problem by using different techniques like search based or machine learning techniques, among others[ref]. Some of the works focused on optimizing compilers in term of execution time. Some others focused on reducing the energy consumption of running programs on hardware machines.

In this work, we aim to provide a new alternative for choosing effective compiler optimization options. In fact, since the search space of possible combinations is too large, we aim to use a new search based technique called Novelty Search to tackle this issue. The idea of this approach is to explore the search space of possible compiler flag options without regard to any objective. In fact, instead of having a fitness-based selection that aim to optimize either execution time or energy consumption, we select optimization sequences based on a novelty score showing how different they are compared to all other combinations evaluated so far. This makes the tool fitness and program independent.

We think also that the search toward effective optimization sequences is not straightforward since the interactions between optimizations is too complex and difficult to predict and to define. In a previous work for example, Chen et al. \cite{chen2012deconstructing}showed that a handful optimizations may lead to higher speedup than other techniques of iterative optimization. In fact, the fitness-based search may be trapped into some local optima that can not escape. This phenomenon is known as "diversity loss". For example, if the most effective optimization sequence that induces less execution time, lies far from the search space defined by the gradient of the fitness function, then some promising search areas may not be reached. The issue of premature convergence to local optima has been a common problem in evolutionary algorithms. Many methods are proposed to overcome this
problem. However, all these alternatives use a fitness-based selection to guide the search. 
Therefore, diversity maintenance in the population level is key for avoiding premature convergence. Considering diversity as the unique objective function to be optimized may be a key solution to this problem.

So during the evolutionary process, we use to select optimization sequences that remain in sparse regions of the search space in order to guide the search through novelty. In the meanwhile, we choose to gather non-functional metrics of explored sequences namely memory and CPU consumption. These metrics will provide us a more fine-grained understanding and analysis of compiler's behavior regarding optimizations. 
\begin{algorithm}
	\caption{Novelty search algorithm for compiler optimizations exploration}
	\label{alg1}
	\begin{algorithmic}[1]
		\REQUIRE Optimization sequences S
		\REQUIRE Benchmark programs Benchmark
		
		\REQUIRE Novelty threshold T
		\REQUIRE Limit L
		\REQUIRE Nearest neighbors K
		
		\ENSURE Best optimization sequence best\_sequence
		\STATE $initialize\_archive(archive,L)$
		\STATE $population \gets random\_sequences(S)$
		\REPEAT
		
		\FOR{$sequence \in population$}   
		\FOR{$program \in benchmark$}
		\STATE 	$perfomance \gets execute(sequence,program)$
		\ENDFOR
		\STATE	$novelty\_metric(sequence) \gets distFromKnearest(archive,population,K)$
		
		\IF{$novelty\_metric > T$}
		\STATE	$archive \gets archive \cup sequence$
		\ENDIF
		
		\ENDFOR
		
		\STATE		$new\_population \gets generate\_new\_population(population)$
		\STATE		$generation \gets generation + 1$
		\UNTIL{generation = N}
		\RETURN best\_sequence
	\end{algorithmic}
\end{algorithm}
\subsubsection{Optimization sequences representation}
For our case study, a candidate solution represents all compiler switches that are used in the 4 standard optimization levels (O1, O2, O3 and Ofast). Thereby, we represent this solution as a vector where each dimension is a compiler flag. The variables which represent compiler options are represented as genes in a chromosome. So, a solution represents the CFLAGS value used by GCC to compile programs.
A solution has always the same size which is the total number of involved flags. But, during the evolutionary process, these flags are turned on or off depending on the mutation and crossover operators. As well, we keep the same order of invoking compiler flags since that does not affect the optimization process and it is handled internally by GCC.

\subsubsection{Novelty Metric}
The Novelty metric expresses the sparseness of an input optimization sequence. It measures its distance to all other sequences in the current population and to all sequences that were discovered in the past (i.e., sequences in the archive).
This measure expresses how unique the optimization sequence is. We can
quantify the sparseness of a solution as the average
distance to the k-nearest neighbors. If the average distance to
a given point's nearest neighbors is large then it belongs to
a sparse area and will get a high novelty score. Otherwise,
if the average distance is small so it belongs certainly to
a dense region then it will get a low novelty score. The distance between two sequences is computed as the total number of symmetric differences among optimization options. Formally, we define this distance as follows :
\begin{equation}
distance(S1,S2)=\left | S1 \bigtriangleup S2 \right |
\end{equation}
where $S1$ et $S2$ are two selected optimization sequences (solutions), In this equation, we calculate the cardinality of the symmetric difference between the two sequences. This distance will be 0 if two optimization sequences are similar and higher than 0 if there is at least one optimization difference. The maximum distance is equal to the total number of input flags.

To measure the sparseness of a solution, we will use the previously defined distance to compute the average distance of a sequence to its k-nearest neighbors. In this context, we define the novelty metric of a particular solution as follows:
\begin{equation}
NM(S) = \frac{1}{k} \sum_{i=1}^{k} distance(S,\mu _{i})
\end{equation}
where $\mu _{i}$ is the $i^{th}$ nearest neighbor of the solution S within the population and the archive of novel individuals.