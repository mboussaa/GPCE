\section{Introduction}

\enlargethispage{0.5cm}


Nowadays, the intensive use of generative programming techniques has become a common practice for software development to tame the runtime platforms heterogeneity that exists in several domains such as mobile or Internet of Things development.  Generative programming techniques reduce the development and maintenance effort by developing at a higher-level of abstraction through the use of domain-specific languages~\cite{brambilla2012model} (DSLs) for example. 
%DSLs, as opposed to general-purpose languages, are software languages that focus on specific problem domains. Thus, the realization of model-driven software development for a specific domain requires the creation of effective code generators and compilers for these DSLs.
%Based on a set of common abstractions, code-generation environments automate and systematize the process of building software systems which can clearly reduce the effort of software implementation.
%The use of code generators is needed to transform manually designed models to software artifacts, which can be deployed on different target platforms. 
A code generator can be used to transform source code programs/models represented in a graphical/textual  language to general purpose programming languages such as C, Java, C++, PHP, JavaScript etc. In turn, generated code is transformed into machine code (binaries) using a set of specific compilers.
%These compilers serve as a basis to target different ranges of platforms. 

%Many technologies, such as Docker containers, provide new opportunities to automate the deployment of produced code into a distributed and heterogeneous component-based infrastructure

However, code generators are known to be difficult to understand since they involve a set of complex and heterogeneous technologies which make the task of performing design, implementation, and testing very hard and time-consuming~\cite{france2007model,guana2015developers}. Code generators have to respect different requirements which preserve software reliability and quality. In fact, faulty code generators can generate defective software artifacts which range from uncompilable or semantically dysfunctional code that causes serious damage to the target platform, to non-functional bugs which lead to poor-quality code that can affect system reliability and performance (e.g., high resource usage, high execution time, etc.). 
%Thus, these issues should be corrected by hand each time the code generator triggers functional or non-functional failures.

In order to check the correctness of the code generation process, developers often use to define (at design time) a set of test cases that verify the functional outcome of generated code. After code generation, test suites are executed within each target platform. This may lead to either a correct behavior (i.e., expected output) or a failure (i.e., crashes, bugs). On the other hand, for performance testing of code generators, developers need to deploy and execute software artifacts within different execution platforms. Then, they have to collect and compare information about the performance and efficiency of the generated code. Finally, they report issues related to the code generation process such as incorrect typing, memory management leaks, etc. Currently there is a lack of automatic solution to check the performance issues such as huge memory/CPU consumption of the generated code. In fact, developers use manually several platform-specific profilers, trackers, and monitoring tools~\cite{guana2014chaintracker,delgado2004taxonomy} in order to find some inconsistencies or bugs during code execution. Ensuring the code quality of generated code can refer to several non-functional properties such as code size, resource or energy consumption, execution time, among others~\cite{pan2006fast}. %Comparing the results is often complex as it does not exist a general benchmark that compares the execution of C, Java or JavaScript program. Consequently, 
Testing the non-functional properties of code generators remains a challenging and time-consuming task because developers have to analyze and verify code for each target platform using platform-dependent tools.

This paper is based on the intuition that we can benefit that a code generator is often a member of a code generators family to automatically compare the  performance of the different generated codes that comes from the same source program. Based on this comparison, we can automatically detect singular resource consumptions that could reveal a code generators bug. As a result we propose an approach to automatically compare and detect inconsistencies between code generators. This approach provides a runtime monitoring infrastructure based on system containers, as execution platforms, to compare the generated code performance. %In fact, we provide a fine-grained understanding of resource consumption and analysis of components behavior. 
We evaluate our approach by analyzing the non-functional properties of HAXE code generator. Haxe is a popular high-level programming language\footnote{1442 github stars} that involves a family of cross-platform code generators able to generate code to different targeted platforms. Our experimental results show that our approach is able to detect performance inconsistencies within HAXE code generators.


In this paper, we make the following contributions:
\begin{itemize} 	
	
	\item We propose a fully automated micro-service infrastructure to ensure the deployment and monitoring of generated code. This paper focuses on the relationship between runtime execution of generated code and resource consumption profiles (memory usage).
	\item We also report the results of an empirical study by evaluating the non-functional properties of HAXE code generators. The obtained results provide evidence to support the claim that our proposed infrastructure is effective.	
\end{itemize}

The paper is organized as follows.
Section II describes the motivations behind this work by discussing three examples of code generator families. Section III presents an overview of our approach to automatically perform non-functional tests of code generator families. In particular, we present our infrastructure for non-functional testing of code generators using micro-services. 
The evaluation and results of our experiments are discussed in Section IV. 
Finally, related work, concluding remarks and future work are provided in Sections V and VI.

