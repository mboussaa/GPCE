\begin{abstract}
The intensive use of generative programming techniques provides an elegant engineering solution to deal with the heterogeneity of platforms and technological stacks. The use of domain-specific languages for example, leads to the creation of numerous code generators that automatically translate high-level system specifications into multi-target executable code. 
Producing correct and efficient code generator is complex and error-prone. Although software designers provide generally high-level test suites to verify the functional outcome of generated code, it remains challenging and tedious to verify the behavior of produced code in terms of non-functional properties.
This paper describes a practical approach based on a runtime monitoring infrastructure to automatically check the potential inefficient code generators. This infrastructure, based on system containers as execution platforms, allows code-generator developers to evaluate the generated code performance.
We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. 
Experimental results show that our approach is able to detect some performance inconsistencies that reveal real issues in Haxe code generators.
\end{abstract}
 

\category{D.3.4}{Programming Languages}{Processorsâ€“ compilers, code generation}

% general terms are not compulsory anymore,
% you may leave them out
\terms
Generative Programming, Testing, Components

\keywords
code quality, non-functional properties, code generator, testing